---
phase: 34-metrics-measurement-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_metrics.py
  - tests/test_cake_stats.py
autonomous: true

must_haves:
  truths:
    - "metrics.py has >=90% test coverage"
    - "MetricsRegistry operations (set/get gauge, inc/get counter) all tested"
    - "MetricsServer HTTP endpoints (/metrics, /health, 404) tested"
    - "All 6 record_* functions tested with correct metric output"
    - "steering/cake_stats.py has >=90% test coverage"
    - "CAKE stats JSON and text parsing tested"
    - "Delta calculation tested (first read, subsequent reads)"
  artifacts:
    - path: "tests/test_metrics.py"
      provides: "Comprehensive metrics module tests"
      min_lines: 200
    - path: "tests/test_cake_stats.py"
      provides: "CAKE stats reader tests"
      min_lines: 150
  key_links:
    - from: "tests/test_metrics.py"
      to: "src/wanctl/metrics.py"
      via: "import and test all public functions"
      pattern: "from wanctl.metrics import"
    - from: "tests/test_cake_stats.py"
      to: "src/wanctl/steering/cake_stats.py"
      via: "import and test CakeStatsReader"
      pattern: "from wanctl.steering.cake_stats import"
---

<objective>
Add comprehensive test coverage for metrics.py (26% -> 90%) and steering/cake_stats.py (24% -> 90%).

Purpose: Achieve MEAS-01 through MEAS-04 requirements. Metrics module is critical for observability; CAKE stats are essential for congestion detection.

Output: Two new test files covering all untested paths in metrics collection and CAKE statistics parsing.
</objective>

<execution_context>
@/home/kevin/.claude/get-shit-done/workflows/execute-plan.md
@/home/kevin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/34-metrics-measurement-tests/34-RESEARCH.md

@src/wanctl/metrics.py
@src/wanctl/steering/cake_stats.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create comprehensive metrics.py tests</name>
  <files>tests/test_metrics.py</files>
  <action>
Create tests/test_metrics.py with comprehensive coverage for metrics.py:

**MetricsRegistry tests:**
- test_set_gauge_stores_value: Set gauge, verify get_gauge returns value
- test_set_gauge_with_labels: Set gauge with labels dict, verify key format
- test_set_gauge_with_help_text: Set gauge with help_text, verify stored
- test_get_gauge_returns_none_for_missing: Non-existent gauge returns None
- test_inc_counter_increments: Call inc_counter multiple times, verify total
- test_inc_counter_with_value: inc_counter with value=5, verify increment
- test_inc_counter_with_help_text: inc_counter with help_text, verify stored
- test_get_counter_returns_none_for_missing: Non-existent counter returns None
- test_make_key_without_labels: Returns just name
- test_make_key_with_labels: Returns name{label="value"} format
- test_extract_base_name_with_labels: Strips labels, returns name
- test_extract_base_name_without_labels: Returns name unchanged
- test_exposition_empty_registry: Empty registry returns empty string
- test_exposition_with_gauges: Verify # TYPE gauge, metric value format
- test_exposition_with_counters: Verify # TYPE counter format
- test_exposition_with_help_text: Verify # HELP line present
- test_exposition_sorted_output: Metrics emitted in sorted order
- test_reset_clears_all: After reset, get_gauge/get_counter return None
- test_thread_safety_concurrent_gauge: 10 threads updating same gauge, no errors

**MetricsHandler tests (requires starting server):**
- test_metrics_endpoint_returns_200: GET /metrics returns 200
- test_metrics_endpoint_content_type: Response has text/plain content type
- test_metrics_endpoint_contains_metrics: Response contains set metrics
- test_health_endpoint_returns_ok: GET /health returns 200 with "OK"
- test_unknown_path_returns_404: GET /unknown returns 404

**MetricsServer tests:**
- test_server_start_returns_true: Fresh start returns True
- test_server_start_already_running: Double start returns False
- test_server_stop_graceful: stop() shuts down cleanly
- test_server_is_running_property: is_running reflects state
- test_server_port_in_use: OSError logged when port unavailable
- test_start_metrics_server_convenience: Convenience function works

**record_* function tests:**
- test_record_autorate_cycle_sets_all_metrics: Verify all gauges/counters set
- test_record_rate_limit_event: Counter incremented with correct labels
- test_record_router_update: Counter incremented
- test_record_ping_failure: Counter incremented
- test_record_steering_state: Gauges set (enabled, congestion state)
- test_record_steering_transition: Counter incremented with from/to labels

**IMPORTANT patterns:**
- Call metrics.reset() in setup/teardown to avoid pollution
- Use unique ports per test class (19100, 19101, etc.)
- Always use try/finally with server.stop() for cleanup
- Use urllib.request for HTTP client (stdlib, no extra dep)
- Test thread safety with threading module
  </action>
  <verify>
.venv/bin/pytest tests/test_metrics.py -v --tb=short
.venv/bin/pytest tests/test_metrics.py --cov=wanctl.metrics --cov-report=term-missing | grep "wanctl/metrics"
  </verify>
  <done>
- tests/test_metrics.py exists with 30+ test methods
- All tests pass
- metrics.py coverage >= 90%
  </done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive cake_stats.py tests</name>
  <files>tests/test_cake_stats.py</files>
  <action>
Create tests/test_cake_stats.py with comprehensive coverage for steering/cake_stats.py:

**CakeStats dataclass tests:**
- test_cake_stats_defaults: All fields default to 0
- test_cake_stats_custom_values: Can construct with custom values

**CongestionSignals dataclass tests:**
- test_congestion_signals_str: __str__ produces expected format

**CakeStatsReader initialization tests:**
- test_init_creates_router_client: Mock get_router_client_with_failover called
- test_init_empty_previous_stats: previous_stats dict initialized empty

**_parse_json_response tests:**
- test_parse_json_list_response: Parse [{"packets": 100, ...}] format
- test_parse_json_dict_response: Parse {"packets": 100, ...} format
- test_parse_json_empty_list: Empty list returns None, logs warning
- test_parse_json_invalid_json: Invalid JSON returns None
- test_parse_json_not_dict: Non-dict data returns None, logs error
- test_parse_json_hyphenated_fields: queued-packets mapped to queued_packets

**_parse_text_response tests:**
- test_parse_text_full_output: All fields extracted from typical SSH output
- test_parse_text_missing_fields: Missing fields default to 0
- test_parse_text_large_numbers: Handles large counter values (2^40+)

**_calculate_stats_delta tests:**
- test_delta_first_read: First read returns current, stores baseline
- test_delta_subsequent_read: Returns diff from previous
- test_delta_cumulative_vs_instantaneous: packets/bytes/dropped are delta, queued_* are current
- test_delta_stores_previous: previous_stats updated after read

**read_stats tests:**
- test_read_stats_success_json: JSON response parsed and delta calculated
- test_read_stats_success_text: Text response parsed and delta calculated
- test_read_stats_invalid_queue_name: ConfigValidationError for invalid name
- test_read_stats_command_failure: Returns None on rc != 0
- test_read_stats_parse_exception: Returns None on parse error, logs debug

**IMPORTANT patterns:**
- Mock get_router_client_with_failover to return MagicMock router client
- Mock client.run_cmd to return (rc, stdout, stderr)
- Use MagicMock for logger fixture
- Test both JSON (REST) and text (SSH) parsing paths
- Verify delta calculation stores previous stats correctly
  </action>
  <verify>
.venv/bin/pytest tests/test_cake_stats.py -v --tb=short
.venv/bin/pytest tests/test_cake_stats.py --cov=wanctl.steering.cake_stats --cov-report=term-missing | grep "cake_stats"
  </verify>
  <done>
- tests/test_cake_stats.py exists with 20+ test methods
- All tests pass
- steering/cake_stats.py coverage >= 90%
  </done>
</task>

</tasks>

<verification>
```bash
# Verify both test files exist and pass
.venv/bin/pytest tests/test_metrics.py tests/test_cake_stats.py -v

# Verify coverage targets met
.venv/bin/pytest tests/test_metrics.py --cov=wanctl.metrics --cov-report=term-missing
.venv/bin/pytest tests/test_cake_stats.py --cov=wanctl.steering.cake_stats --cov-report=term-missing

# Both should show >= 90% coverage
```
</verification>

<success_criteria>
- [ ] tests/test_metrics.py created with comprehensive MetricsRegistry, MetricsServer, record_* tests
- [ ] tests/test_cake_stats.py created with JSON/text parsing, delta calculation tests
- [ ] metrics.py coverage >= 90% (from 26%)
- [ ] steering/cake_stats.py coverage >= 90% (from 24%)
- [ ] All tests pass
- [ ] MEAS-01, MEAS-02, MEAS-03, MEAS-04 requirements satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/34-metrics-measurement-tests/34-01-SUMMARY.md`
</output>
