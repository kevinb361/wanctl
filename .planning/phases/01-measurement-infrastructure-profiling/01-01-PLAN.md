---
phase: 01-measurement-infrastructure-profiling
plan: 01
type: execute
---

<objective>
Profile the current measurement cycle latency and create instrumentation foundation for performance optimization.

Purpose: Understand where time is spent in each measurement subsystem (RouterOS communication, ICMP ping, CAKE stats collection) so optimization efforts can be prioritized effectively.

Output: Profiling data showing latency breakdown + instrumentation module enabling future cycle timing measurements
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/codebase/ARCHITECTURE.md
@src/wanctl/autorate_continuous.py
@src/wanctl/steering/daemon.py
@src/wanctl/rtt_measurement.py
@src/wanctl/steering/cake_stats.py

**Architecture Context:**
- Measurement layer consists of RTT measurement (ICMP ping) and CAKE stats reader
- Current bottlenecks documented: RouterOS SSH ~150ms, Ping ~100-150ms, CAKE stats ~50-100ms
- Dual-transport (REST API preferred, SSH fallback) with different latencies
- All three measurements run every cycle (no caching yet)
- 2-second steering cycles vs configurable autorate cycles (default 10 min)

**Constraining Decisions:**
- Production stability first (profiling must not destabilize daemon)
- No external dependencies (profile data locally, no monitoring services)
- Backward compatible (existing deployments must continue working)
- Python 3.12 only

**Established Patterns:**
- Logging via logger parameter, structured log messages
- Error handling with try/except + logging
- Configuration-driven via YAML files
- State persistence via StateManager
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create performance profiling utility module</name>
  <files>src/wanctl/perf_profiler.py</files>
  <action>
Create a new module `src/wanctl/perf_profiler.py` that provides instrumentation for measuring operation latencies. Include:

1. **PerfTimer class** - Context manager for timing code blocks:
   - `__init__(label: str, logger: Optional[logging.Logger] = None)`
   - `__enter__()` - Start timer
   - `__exit__()` - Record elapsed time (milliseconds), log result
   - Example: `with PerfTimer("ping_measurement", logger): ...`
   - Logs: "{label}: {elapsed_ms:.1f}ms"

2. **OperationProfiler class** - Accumulate timing across multiple cycles:
   - `__init__(max_samples: int = 100)` - Keep last N measurements
   - `record(label: str, elapsed_ms: float)` - Add measurement
   - `stats(label: str) -> dict` - Return {count, min_ms, max_ms, avg_ms, p95_ms, p99_ms}
   - `clear(label: str = None)` - Clear specific label or all
   - `report(logger: logging.Logger = None)` - Log summary of all recorded metrics

3. **Functions:**
   - `measure_operation(func, label: str, logger: Optional[logging.Logger] = None) -> Any` - Decorator for timing function calls
   - Example: `@measure_operation("read_cake_stats")` on a method

**Implementation notes:**
- Use time.perf_counter() for high-precision timing (not time.time())
- Store samples in deques (maxlen=100) to prevent unbounded growth
- Calculate percentiles using sorted list of samples
- All logging optional (logger parameter), should gracefully handle None logger
- No external dependencies beyond Python stdlib (statistics module for median)
- Type hints for all functions/classes

**Files created:** 1
**Approximate lines:** 180-200
  </action>
  <verify>
python -c "from src.wanctl.perf_profiler import PerfTimer, OperationProfiler; print('Import successful')"
pytest tests/test_perf_profiler.py -v (if unit tests exist)
  </verify>
  <done>
Module imports without errors, PerfTimer context manager works, OperationProfiler accumulates samples, report() generates output without crashing
  </done>
</task>

<task type="auto">
  <name>Task 2: Instrument measurement cycle with profiling hooks</name>
  <files>src/wanctl/steering/daemon.py, src/wanctl/autorate_continuous.py</files>
  <action>
Add instrumentation to existing measurement code to capture timing data. Do NOT refactor code, only add timing hooks.

**In src/wanctl/steering/daemon.py:**
1. Import PerfTimer at top
2. Wrap RTT measurement in run_cycle() with `with PerfTimer("steering_rtt_measurement", self.logger):`
3. Wrap CAKE stats read with `with PerfTimer("steering_cake_stats_read", self.logger):`
4. Wrap RouterOS rule state check with `with PerfTimer("steering_rule_check", self.logger):`
5. Wrap entire cycle with `with PerfTimer("steering_cycle_total", self.logger):` around whole measurement section
6. Log breakdown at end of cycle (e.g., "Steering cycle: RTT=12.3ms, CAKE=45.2ms, Rule=5.1ms, Total=62.6ms")

**In src/wanctl/autorate_continuous.py:**
1. Import PerfTimer at top
2. Wrap RTT measurement with `with PerfTimer("autorate_rtt_measurement", self.logger):`
3. Wrap CAKE stats read with `with PerfTimer("autorate_cake_stats_read", self.logger):`
4. Wrap state adjustment with `with PerfTimer("autorate_adjust_state", self.logger):`
5. Wrap entire cycle with `with PerfTimer("autorate_cycle_total", self.logger):` around whole measurement section

**Implementation approach:**
- Add imports (PerfTimer) to existing imports sections
- Wrap existing code blocks, do not move or refactor them
- Use meaningful labels matching subsystem names
- Log individual subsystem times in cycle log output for observability
- Ensure all existing functionality unchanged (no behavioral modifications)

**Files modified:** 2
**Approximate lines added:** 12 lines per file (imports + wraps)
  </action>
  <verify>
grep -n "PerfTimer" src/wanctl/steering/daemon.py
grep -n "PerfTimer" src/wanctl/autorate_continuous.py
journalctl -u wanctl@* -n 20 --no-pager | grep -E "ms|Cycle" (check logs for timing output)
  </verify>
  <done>
Both files import PerfTimer, all measurement subsystems wrapped with timing hooks, logs show latency breakdown per cycle (RTT measurement ms, CAKE stats ms, total ms)
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python -m py_compile src/wanctl/perf_profiler.py` - No syntax errors
- [ ] `python -c "from src.wanctl import perf_profiler; print(perf_profiler.PerfTimer)"` - Module imports
- [ ] Existing test suite still passes: `pytest tests/ -v`
- [ ] Steering daemon can start with profiling hooks: `python -m wanctl.steering.daemon --config configs/steering.yaml.example` (dry-run)
- [ ] Autorate can start with profiling hooks: `python -m wanctl.autorate_continuous --config configs/wan1.yaml.example --check` (validation only)
- [ ] No behavioral changes: logs show same congestion decisions/state transitions as before
</verification>

<success_criteria>

- Profiling utility module created and imports successfully
- No syntax errors in modified files
- Existing test suite passes (no regressions)
- Measurement hooks in place (can verify via grep)
- Daemons can start without errors
- Profiling output appears in logs showing timing breakdown
  </success_criteria>

<output>
After completion, create `.planning/phases/01-measurement-infrastructure-profiling/01-01-SUMMARY.md`:

# Phase 1 Plan 1 Summary: Measurement Infrastructure Profiling - Instrumentation Foundation

**Profiling utility module created and integrated into existing measurement cycle for latency visibility.**

## Accomplishments

- Created `perf_profiler.py` module with PerfTimer context manager for operation timing
- Integrated timing hooks into steering daemon measurement cycle (RTT, CAKE, rule check)
- Integrated timing hooks into autorate daemon measurement cycle (RTT, CAKE, state adjustment)
- All measurement subsystems now report individual latency metrics in logs

## Files Created/Modified

- `src/wanctl/perf_profiler.py` - New profiling utility module (PerfTimer, OperationProfiler classes)
- `src/wanctl/steering/daemon.py` - Added timing instrumentation (4 hooks in run_cycle)
- `src/wanctl/autorate_continuous.py` - Added timing instrumentation (4 hooks in main cycle)

## Decisions Made

- Used perf_counter() for high-precision timing vs system clock (wall clock time not needed for relative measurements)
- Deque-based sample storage (maxlen=100) to prevent unbounded memory growth in long-running daemons
- Optional logger parameter to handle None gracefully (no crashes if profiler used without logger)
- Logging via existing logger instances (no new dependencies or logger setup)

## Issues Encountered

None - implementation straightforward, hooks added non-invasively to existing code

## Next Step

Ready for 01-02-PLAN.md: Profiling cycle data collection and analysis
</output>
