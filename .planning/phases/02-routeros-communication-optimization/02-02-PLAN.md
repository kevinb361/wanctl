---
phase: 02-interval-optimization
plan: 02
type: execute
---

<objective>
Test 100ms cycle interval for aggressive congestion response while maintaining stability.

Purpose: Validate 5x faster response than original (500ms → 100ms) before attempting extreme 50ms interval. This represents a significant jump requiring careful EWMA tuning.
Output: 100ms interval deployed if 250ms was stable, stability validated over 24-48h, decision on whether to proceed to 50ms.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@docs/FASTER_RESPONSE_INTERVAL.md
@docs/INTERVAL_TESTING_250MS.md
@.planning/phases/02-interval-optimization/02-01-SUMMARY.md
@src/wanctl/autorate_continuous.py
@configs/spectrum.yaml
@configs/att.yaml
@configs/steering_config_v2.yaml

**Prerequisites:**
- Plan 02-01 complete with "approved" status
- 250ms interval proven stable

**Current state (after 02-01):**
- Cycle interval: 250ms
- Congestion detection: 16 samples × 250ms = 4 seconds
- Recovery: 120 samples × 250ms = 30 seconds

**Target for 100ms:**
- Congestion detection: 40 samples × 100ms = 4 seconds (preserve time constant)
- Recovery: 300 samples × 100ms = 30 seconds (preserve time constant)
- Expected CPU: ~25-30% (2.5x from 250ms, 10x from original 1s)

**Critical consideration:**
100ms interval means 10 cycles per second. EWMA smoothing must be strong enough to prevent measurement noise from causing flapping.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Calculate and apply 100ms interval parameters</name>
  <files>src/wanctl/autorate_continuous.py, configs/spectrum.yaml, configs/att.yaml, configs/steering_config_v2.yaml, src/wanctl/steering/daemon.py</files>
  <action>
Update cycle interval and all dependent parameters for 100ms operation:

1. **Autorate interval:** CYCLE_INTERVAL_SECONDS = 0.1 (from 0.25)

2. **EWMA alphas (preserve time constants via 2.5x reduction from 250ms):**
   - Spectrum: alpha_baseline: 0.001 (from 0.0025), alpha_load: 0.01 (from 0.025)
   - ATT: alpha_baseline: 0.00075 (from 0.001875), alpha_load: 0.01 (from 0.025)

3. **Steering thresholds (preserve time-based intent via 2.5x sample counts):**
   - interval_seconds: 0.1 (from 0.25)
   - red_samples_required: 40 (from 16) - maintains 4 second activation
   - green_samples_required: 300 (from 120) - maintains 30 second recovery
   - history_size: 1200 (from 480) - maintains 2 minute history window

4. **Steering daemon MAX_HISTORY_SAMPLES:** 1200 (from 480)

**Mathematical verification:**
- Baseline smoothing: 1000 samples × 0.1s = 100 seconds (preserved)
- Load smoothing: 100 samples × 0.1s = 10 seconds (preserved)
- RED activation: 40 × 0.1s = 4 seconds (preserved)
- GREEN recovery: 300 × 0.1s = 30 seconds (preserved)

**Critical:** At 100ms, EWMA alpha values are very small (0.001, 0.01). This provides strong smoothing to prevent noise-induced flapping. The baseline alpha of 0.001 means each new sample contributes only 0.1% to the average - very stable.

**What to avoid:** Do NOT increase alpha values to "respond faster" - the interval already provides fast response. Strong smoothing prevents instability.
  </action>
  <verify>
1. `python3 -m py_compile src/wanctl/autorate_continuous.py` passes
2. `python3 -m py_compile src/wanctl/steering/daemon.py` passes
3. Config files are valid YAML
4. Alpha values: 0.00075 ≤ alpha_baseline ≤ 0.001, alpha_load = 0.01
5. Sample counts: red=40, green=300, history=1200
  </verify>
  <done>
- Interval set to 0.1 in autorate
- EWMA alphas reduced by 2.5x from 250ms (time constants preserved)
- Steering thresholds increased 2.5x (time-based intent preserved)
- History sizes increased 2.5x (2-min window preserved)
- Strong EWMA smoothing configured to prevent noise-induced flapping
  </done>
</task>

<task type="auto">
  <name>Task 2: Deploy 100ms configuration and monitor initial stability</name>
  <files>Deployment via systemd</files>
  <action>
Deploy updated configuration to production:

1. Copy updated files to containers (same process as 02-01 Task 2)
2. Restart services
3. Monitor first 1-2 hours closely for immediate issues

**Initial monitoring (first 2 hours):**
- Watch for immediate flapping or oscillation
- Verify services don't crash from high polling rate
- Check router CPU doesn't spike >50%
- Confirm logs show 100ms cycle timing

If immediate issues found (crashes, severe flapping, router overload):
- Rollback to 250ms immediately
- Document failure mode
- Skip to Task 3 with "rollback" recommendation

If initial 2 hours stable:
- Continue to 24-48h monitoring checkpoint
  </action>
  <verify>
1. Services running for >2 hours without crashes
2. No severe flapping in first 100 cycles
3. Router CPU stable (<50%)
4. Log entries show ~100ms cycle timing
5. No error spikes in soak-monitor.sh
  </verify>
  <done>
- 100ms interval deployed
- Initial 2-hour stability check passed
- Ready for extended 24-48h monitoring
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>100ms cycle interval deployed and initial stability validated</what-built>
  <how-to-verify>
Monitor for 24-48 hours with focus on aggressive interval challenges:

1. **Baseline RTT stability (CRITICAL - harder at 100ms):**
   - `ssh cake-spectrum 'journalctl -u wanctl@spectrum --since "24 hours ago" | grep baseline | tail -100'`
   - At 100ms, small measurement variance is amplified 10x vs original 1s
   - Confirm: baseline_rtt still stable (no drift >5ms under load)
   - Look for: Smooth baseline despite 10x sampling rate

2. **No flapping from measurement noise:**
   - `ssh cake-spectrum 'journalctl -u wanctl@spectrum --since "24 hours ago" | grep -E "download_rate" | tail -100'`
   - Critical: EWMA smoothing (alpha=0.001/0.01) must filter noise
   - Confirm: No spurious rate changes from normal RTT jitter (±2-3ms)
   - Pattern: Rate changes should correlate with actual congestion, not noise

3. **State transition timing:**
   - `ssh cake-spectrum 'journalctl -u wanctl-steering --since "24 hours ago" | grep -E "(YELLOW|RED)" | head -20'`
   - Confirm: RED still requires 40 consecutive bad samples (~4 seconds)
   - Verify: Not triggering RED from brief RTT spikes (<1 second duration)

4. **Router CPU and memory:**
   - 100ms = 10 REST API calls per second to router
   - Check: `/system resource print` on router
   - Target: CPU <30%, memory stable
   - If router struggles: 100ms may be router hardware limit

5. **Network measurement quality:**
   - ICMP ping 3x per 100ms = 30 pings/second to 1.1.1.1
   - Verify: Ping responses remain consistent
   - Check for: Dropped pings, timeout increases

6. **Cycle execution time stability:**
   - `ssh cake-spectrum 'journalctl -u wanctl@spectrum --since "1 hour ago" | grep cycle | tail -50'`
   - Confirm: Cycle execution still 30-45ms (not degrading)
   - 100ms interval leaves ~55-70ms margin - should be adequate

7. **Compare to 250ms baseline:**
   - Is 100ms measurably better at congestion response?
   - Or is 250ms already "fast enough" with lower risk?
  </how-to-verify>
  <resume-signal>
Type one of:
- "approved" - 100ms stable, consider 50ms test
- "stable-but-finalize" - 100ms works, but finalize here (don't push to 50ms)
- "issues: [description]" - problems found, need adjustment
- "rollback-to-250ms" - 100ms unstable, 250ms is optimal
  </resume-signal>
</task>

<task type="checkpoint:decision" gate="blocking">
  <decision>Proceed to 50ms extreme testing or finalize at 100ms?</decision>
  <context>
100ms interval has been validated stable. The question is whether to push further to 50ms (20 cycles/second, extremely aggressive) or finalize at 100ms.

**Considerations:**
- 100ms provides 4-second congestion detection (very fast for bufferbloat)
- 50ms would provide 2-second detection (marginal improvement)
- 50ms = 20 router API calls/second (potential router strain)
- 50ms = 60 pings/second to reflectors (high network overhead)
- Diminishing returns: human perception of <1s difference is minimal
- Risk: 50ms may be unstable even with perfect EWMA tuning
  </context>
  <options>
    <option id="test-50ms">
      <name>Proceed to 50ms testing (Plan 02-03)</name>
      <pros>Maximum possible responsiveness, explore absolute performance limit, scientific value</pros>
      <cons>High risk of instability, router may not handle load, minimal practical benefit over 100ms, potential production disruption</cons>
    </option>
    <option id="finalize-100ms">
      <name>Finalize at 100ms (skip Plan 02-03)</name>
      <pros>Excellent responsiveness (4s detection), proven stable, safe for production, 10x improvement over original</pros>
      <cons>Won't know if 50ms is possible, leaves potential performance on table</cons>
    </option>
    <option id="finalize-250ms">
      <name>Rollback to 250ms and finalize</name>
      <pros>More conservative, lower router load, larger stability margin</pros>
      <cons>Slower response than 100ms (8s vs 4s detection), settling for less when more is possible</cons>
    </option>
  </options>
  <resume-signal>Select: test-50ms, finalize-100ms, or finalize-250ms</resume-signal>
</task>

<task type="auto">
  <name>Task 3: Document 100ms findings and recommendation</name>
  <files>docs/INTERVAL_TESTING_100MS.md</files>
  <action>
Create analysis document for 100ms test results:

**Structure:**
1. **Test Parameters:** 100ms interval, alpha values (0.001/0.01), thresholds (40/300), test duration
2. **Stability Analysis:**
   - Baseline RTT behavior at 10x sampling rate
   - Flapping incidents and root causes
   - Router CPU/memory impact
   - Measurement quality (ping consistency)
   - EWMA effectiveness at filtering noise
3. **Performance Comparison:**
   - 100ms vs 250ms congestion detection speed
   - 100ms vs 250ms recovery speed
   - Practical benefit analysis
4. **Router Hardware Impact:**
   - CPU usage: [%] (10 REST calls/second sustainable?)
   - Memory: [stable/growing]
   - API responsiveness
5. **Decision Recommendation:**
   - Proceed to 50ms? Or finalize at 100ms/250ms?
   - Rationale based on stability vs. marginal gains

**What to avoid:** Don't recommend 50ms just to "see what happens" - only if there's clear benefit and stability confidence.
  </action>
  <verify>
1. Document exists at docs/INTERVAL_TESTING_100MS.md
2. Contains stability analysis with actual data
3. Includes comparison to 250ms baseline
4. Clear recommendation with rationale
5. Router hardware impact documented
  </verify>
  <done>
- 100ms test results documented
- Stability assessment complete
- Comparison to 250ms provided
- Clear recommendation for next step (50ms test or finalize)
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] 100ms parameters calculated correctly (time constants preserved)
- [ ] 100ms interval deployed and stable for 24-48h
- [ ] No baseline drift or flapping at aggressive sampling rate
- [ ] Router hardware handles 10 API calls/second
- [ ] EWMA smoothing effectively filters measurement noise
- [ ] Decision made: proceed to 50ms or finalize
- [ ] Findings documented in INTERVAL_TESTING_100MS.md
</verification>

<success_criteria>
- 100ms interval operational and validated
- Stability proven over 24-48 hours at aggressive polling rate
- EWMA smoothing prevents noise-induced flapping
- Router CPU impact acceptable
- Decision made on 50ms testing
- Documentation complete
</success_criteria>

<output>
After completion, create `.planning/phases/02-interval-optimization/02-02-SUMMARY.md`:

# Phase 2 Plan 2: 100ms Interval Testing Summary

**100ms interval validated - 10x faster than original baseline, [stable/unstable]**

## Accomplishments

- Calculated 100ms parameters with strong EWMA smoothing
- Deployed to production and monitored for [duration]
- [Validated stability / Identified instability issues]
- Documented findings in INTERVAL_TESTING_100MS.md
- Decision: [Proceed to 50ms / Finalize at 100ms / Rollback to 250ms]

## Files Created/Modified

- `src/wanctl/autorate_continuous.py` - CYCLE_INTERVAL_SECONDS = 0.1
- `configs/spectrum.yaml` - EWMA alphas: 0.001/0.01
- `configs/att.yaml` - EWMA alphas: 0.00075/0.01
- `configs/steering_config_v2.yaml` - Thresholds 40/300, history 1200
- `src/wanctl/steering/daemon.py` - MAX_HISTORY_SAMPLES = 1200
- `docs/INTERVAL_TESTING_100MS.md` - Analysis and recommendation

## Decisions Made

- 100ms stability: [Approved/Issues found]
- Next step: [Test 50ms / Finalize at 100ms / Rollback to 250ms]
- Rationale: [based on stability, performance, router capacity]

## Issues Encountered

[Stability issues, or "None - 100ms stable with strong EWMA smoothing"]

## Next Step

[If approved for 50ms: "Ready for 02-03-PLAN.md (50ms extreme interval test)"]
[If finalize: "Phase 2 complete - finalize at 100ms/250ms interval"]
</output>
