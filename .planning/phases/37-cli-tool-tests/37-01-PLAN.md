---
phase: 37-cli-tool-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_calibrate.py
autonomous: true

must_haves:
  truths:
    - "All CLI flags (11 options) have at least one test exercising them"
    - "CalibrationResult dataclass serializes correctly to dict"
    - "Connectivity tests (SSH, netperf) handle success, failure, and timeout"
    - "RTT measurement parses ping output and calculates baseline"
    - "Binary search algorithm converges to optimal rate"
    - "Config generation writes valid YAML with correct structure"
    - "main() entry point handles args, signals, and exit codes"
  artifacts:
    - path: "tests/test_calibrate.py"
      provides: "Comprehensive calibrate.py test coverage"
      contains: "TestArgumentParsing|TestConnectivity|TestMeasurement|TestBinarySearch|TestConfigGeneration|TestMain"
  key_links:
    - from: "tests/test_calibrate.py"
      to: "wanctl.calibrate.main"
      via: "direct import and mock"
      pattern: "from wanctl.calibrate import"
---

<objective>
Test calibrate.py CLI tool for 90%+ coverage with mocked subprocess calls.

Purpose: Cover calibration wizard including argparse, connectivity tests, RTT measurement, binary search, config generation, and main entry point.
Output: New test file tests/test_calibrate.py with ~40 tests across 6 test classes.
</objective>

<execution_context>
@/home/kevin/.claude/get-shit-done/workflows/execute-plan.md
@/home/kevin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/37-cli-tool-tests/37-CONTEXT.md
@src/wanctl/calibrate.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test file with argument parsing and CalibrationResult tests</name>
  <files>tests/test_calibrate.py</files>
  <action>
Create tests/test_calibrate.py with TestArgumentParsing and TestCalibrationResult classes.

**TestArgumentParsing class:**

1. Required arguments (--wan-name, --router):
   - Missing --wan-name raises SystemExit (use pytest.raises(SystemExit))
   - Missing --router raises SystemExit
   - Both provided: parser succeeds

2. Optional arguments with defaults:
   - --user defaults to "admin"
   - --netperf-host defaults to "netperf.bufferbloat.net"
   - --ping-host defaults to "1.1.1.1"
   - --target-bloat defaults to 10.0
   - --output-dir defaults to "/etc/wanctl"

3. Optional arguments override defaults:
   - --user override works
   - --ssh-key accepts path
   - --download-queue, --upload-queue set custom queue names
   - --target-bloat accepts float
   - --skip-binary-search sets flag True

Pattern for argument parsing tests:
```python
def test_missing_wan_name_raises_error(self):
    with pytest.raises(SystemExit):
        parser = create_test_parser()
        parser.parse_args(["--router", "192.168.1.1"])

def test_all_optional_args(self):
    args = parse_args_safely([
        "--wan-name", "wan1",
        "--router", "192.168.1.1",
        "--user", "custom",
        "--ssh-key", "/path/to/key",
        "--target-bloat", "15.0",
        "--skip-binary-search"
    ])
    assert args.user == "custom"
    assert args.ssh_key == "/path/to/key"
    assert args.target_bloat == 15.0
    assert args.skip_binary_search is True
```

**TestCalibrationResult class:**

1. Test to_dict() serialization:
   - Create CalibrationResult with all fields
   - Call to_dict()
   - Verify all fields present with correct types

2. Test field validation:
   - Verify dataclass creates valid instance
   - Verify timestamp format correct

Helper function to create parser for testing:
```python
def parse_args_safely(args):
    """Helper to parse args without sys.exit."""
    parser = argparse.ArgumentParser()
    # Add same args as main()
    parser.add_argument("--wan-name", required=True)
    parser.add_argument("--router", required=True)
    parser.add_argument("--user", default="admin")
    parser.add_argument("--ssh-key")
    parser.add_argument("--netperf-host", default="netperf.bufferbloat.net")
    parser.add_argument("--ping-host", default="1.1.1.1")
    parser.add_argument("--download-queue")
    parser.add_argument("--upload-queue")
    parser.add_argument("--target-bloat", type=float, default=10.0)
    parser.add_argument("--output-dir", default="/etc/wanctl")
    parser.add_argument("--skip-binary-search", action="store_true")
    return parser.parse_args(args)
```
  </action>
  <verify>`.venv/bin/pytest tests/test_calibrate.py::TestArgumentParsing tests/test_calibrate.py::TestCalibrationResult -v` passes</verify>
  <done>CLI argument parsing and CalibrationResult tests complete with all 11 flags covered</done>
</task>

<task type="auto">
  <name>Task 2: Add connectivity and measurement tests</name>
  <files>tests/test_calibrate.py</files>
  <action>
Add TestConnectivity and TestMeasurement classes.

**TestConnectivity class:**

1. test_ssh_connectivity_success:
   - Mock subprocess.run to return returncode=0, stdout="ok"
   - Call test_ssh_connectivity(host, user)
   - Verify returns True

2. test_ssh_connectivity_failure:
   - Mock subprocess.run to return returncode=1
   - Verify returns False

3. test_ssh_connectivity_timeout:
   - Mock subprocess.run to raise TimeoutExpired
   - Verify returns False

4. test_ssh_connectivity_with_ssh_key:
   - Mock subprocess.run to return success
   - Verify "-i" and key path in command

5. test_netperf_server_success:
   - Mock subprocess.run to return returncode=0
   - Verify returns True

6. test_netperf_server_failure:
   - Mock subprocess.run to return returncode=1
   - Verify returns False

7. test_netperf_server_not_installed:
   - Mock subprocess.run to raise FileNotFoundError
   - Verify returns False

8. test_netperf_server_timeout:
   - Mock subprocess.run to raise TimeoutExpired
   - Verify returns False

**TestMeasurement class:**

1. test_measure_baseline_rtt_success:
   - Mock subprocess.run with valid ping output
   - Verify returns float (minimum RTT)

2. test_measure_baseline_rtt_failure:
   - Mock subprocess.run with returncode=1
   - Verify returns None

3. test_measure_baseline_rtt_timeout:
   - Mock subprocess.run to raise TimeoutExpired
   - Verify returns None

4. test_measure_baseline_rtt_no_samples:
   - Mock subprocess.run with output but no RTT values
   - Verify returns None

5. test_measure_throughput_download:
   - Mock subprocess.Popen for netperf (with stdout containing throughput)
   - Mock subprocess.run for ping (with RTT values)
   - Verify returns (throughput, bloat) tuple

6. test_measure_throughput_upload:
   - Similar to download test
   - Verify returns (throughput, bloat) tuple

7. test_measure_throughput_exception:
   - Mock subprocess.Popen to raise Exception
   - Verify returns (0.0, 0.0)

Mock pattern for subprocess:
```python
@patch("wanctl.calibrate.subprocess.run")
def test_ssh_connectivity_success(self, mock_run):
    mock_run.return_value = MagicMock(returncode=0, stdout="ok\n", stderr="")
    result = test_ssh_connectivity("192.168.1.1", "admin")
    assert result is True
    assert mock_run.called

@patch("wanctl.calibrate.subprocess.run")
def test_ssh_connectivity_timeout(self, mock_run):
    from subprocess import TimeoutExpired
    mock_run.side_effect = TimeoutExpired(cmd="ssh", timeout=5)
    result = test_ssh_connectivity("192.168.1.1", "admin")
    assert result is False
```

Sample ping output for tests:
```python
PING_OUTPUT_SUCCESS = """
PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.
64 bytes from 1.1.1.1: icmp_seq=1 ttl=57 time=12.3 ms
64 bytes from 1.1.1.1: icmp_seq=2 ttl=57 time=11.8 ms
64 bytes from 1.1.1.1: icmp_seq=3 ttl=57 time=12.1 ms

--- 1.1.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 11.800/12.066/12.300/0.205 ms
"""
```
  </action>
  <verify>`.venv/bin/pytest tests/test_calibrate.py::TestConnectivity tests/test_calibrate.py::TestMeasurement -v` passes</verify>
  <done>Connectivity and measurement functions have comprehensive test coverage</done>
</task>

<task type="auto">
  <name>Task 3: Add binary search, config generation, and main entry point tests</name>
  <files>tests/test_calibrate.py</files>
  <action>
Add TestBinarySearch, TestConfigGeneration, and TestMain classes.

**TestBinarySearch class:**

1. test_set_cake_limit_success:
   - Mock subprocess.run to return success
   - Verify returns True

2. test_set_cake_limit_failure:
   - Mock subprocess.run to return failure
   - Verify returns False

3. test_binary_search_converges:
   - Mock set_cake_limit to return True
   - Mock measure_throughput_download to return values that guide search
   - Verify binary_search_optimal_rate returns reasonable values
   - Use simple mock that returns (throughput, bloat) where bloat < target_bloat

4. test_binary_search_skips_on_limit_failure:
   - Mock set_cake_limit to return False
   - Verify iteration skipped with warning

**TestConfigGeneration class:**

1. test_generate_config_success(tmp_path):
   - Create CalibrationResult with test values
   - Call generate_config(result, tmp_path / "test.yaml")
   - Verify file exists
   - Parse YAML and verify structure

2. test_generate_config_fiber_detection:
   - Use baseline_rtt_ms < 15
   - Verify config contains fiber-appropriate alpha_baseline

3. test_generate_config_cable_detection:
   - Use 15 <= baseline_rtt_ms < 35
   - Verify config contains cable-appropriate alpha_baseline

4. test_generate_config_dsl_detection:
   - Use baseline_rtt_ms >= 35
   - Verify config contains dsl-appropriate alpha_baseline

5. test_generate_config_permission_error:
   - Use path that can't be written
   - Verify returns False

**TestMain class:**

1. test_main_success:
   - Mock run_calibration to return valid CalibrationResult
   - Call main() with valid args
   - Verify returns 0

2. test_main_failure:
   - Mock run_calibration to return None
   - Call main() with valid args
   - Verify returns 1

3. test_main_sigint:
   - Mock is_shutdown_requested to return True
   - Mock run_calibration to return None
   - Verify returns 130

4. test_main_registers_signal_handlers:
   - Mock register_signal_handlers
   - Call main()
   - Verify register_signal_handlers called with include_sigterm=False

**TestStepHelpers class (optional but recommended for full coverage):**

1. test_step_connectivity_ssh_failure:
   - Mock test_ssh_connectivity to return False
   - Verify _step_connectivity_tests returns (False, False)

2. test_step_connectivity_netperf_failure:
   - Mock test_ssh_connectivity to return True
   - Mock test_netperf_server to return False
   - Verify returns (True, True) (skip_throughput=True)

3. test_step_baseline_rtt_interrupt:
   - Mock measure_baseline_rtt to return value
   - Mock is_shutdown_requested to return True
   - Verify returns None

4. test_step_raw_throughput_skipped:
   - Call _step_raw_throughput with skip_throughput=True
   - Verify returns defaults (100.0, 20.0, 0.0, 0.0)

Mock pattern for main():
```python
@patch("sys.argv", ["calibrate", "--wan-name", "wan1", "--router", "192.168.1.1"])
@patch("wanctl.calibrate.register_signal_handlers")
@patch("wanctl.calibrate.run_calibration")
def test_main_success(self, mock_run_cal, mock_signals):
    mock_run_cal.return_value = MagicMock()  # Valid result
    result = main()
    assert result == 0
    mock_signals.assert_called_once_with(include_sigterm=False)
```
  </action>
  <verify>`.venv/bin/pytest tests/test_calibrate.py -v` passes and coverage shows 90%+</verify>
  <done>Binary search, config generation, and main entry point have complete test coverage</done>
</task>

</tasks>

<verification>
Run full coverage check:
```bash
.venv/bin/pytest tests/test_calibrate.py --cov=wanctl.calibrate --cov-report=term-missing 2>&1 | grep -E "calibrate.py.*%"
```

Target: 90%+ coverage (from 0%)

Run all tests:
```bash
.venv/bin/pytest tests/test_calibrate.py -v
```
</verification>

<success_criteria>
1. TestArgumentParsing: All 11 CLI flags tested
2. TestCalibrationResult: Dataclass serialization verified
3. TestConnectivity: SSH and netperf tests (success, failure, timeout)
4. TestMeasurement: RTT and throughput measurement tests
5. TestBinarySearch: Binary search algorithm convergence tested
6. TestConfigGeneration: YAML output with connection type detection
7. TestMain: Entry point with exit codes (0, 1, 130)
8. calibrate.py coverage >= 90%
9. All tests pass: `.venv/bin/pytest tests/test_calibrate.py -v`
</success_criteria>

<output>
After completion, create `.planning/phases/37-cli-tool-tests/37-01-SUMMARY.md`
</output>
