---
phase: 37-cli-tool-tests
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_perf_profiler.py
autonomous: true

must_haves:
  truths:
    - "PerfTimer measures elapsed time in milliseconds"
    - "PerfTimer logs to provided logger at DEBUG level"
    - "OperationProfiler accumulates samples with bounded deques"
    - "OperationProfiler.stats() returns min/max/avg/p95/p99"
    - "OperationProfiler.clear() removes samples for label or all"
    - "OperationProfiler.report() generates formatted summary"
    - "measure_operation decorator times function calls"
  artifacts:
    - path: "tests/test_perf_profiler.py"
      provides: "Comprehensive perf_profiler.py test coverage"
      contains: "TestPerfTimer|TestOperationProfiler|TestMeasureOperationDecorator"
  key_links:
    - from: "tests/test_perf_profiler.py"
      to: "wanctl.perf_profiler"
      via: "direct import"
      pattern: "from wanctl.perf_profiler import"
---

<objective>
Test perf_profiler.py timing utilities for 90%+ coverage.

Purpose: Cover PerfTimer context manager, OperationProfiler class, and measure_operation decorator.
Output: New test file tests/test_perf_profiler.py with ~20 tests across 3 test classes.
</objective>

<execution_context>
@/home/kevin/.claude/get-shit-done/workflows/execute-plan.md
@/home/kevin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/37-cli-tool-tests/37-CONTEXT.md
@src/wanctl/perf_profiler.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test file with PerfTimer tests</name>
  <files>tests/test_perf_profiler.py</files>
  <action>
Create tests/test_perf_profiler.py with TestPerfTimer class.

**TestPerfTimer class:**

1. test_timer_measures_elapsed_time:
   - Create PerfTimer context
   - Sleep briefly (0.01s)
   - Verify elapsed_ms > 0 and reasonable (10-50ms range)

2. test_timer_without_logger:
   - Create PerfTimer with logger=None
   - Use context
   - Verify elapsed_ms set, no exception raised

3. test_timer_with_logger:
   - Create mock logger
   - Create PerfTimer with label and logger
   - Use context
   - Verify logger.debug called with label and ms value

4. test_timer_logs_correct_format:
   - Create mock logger
   - Use PerfTimer with label "test_op"
   - Verify log message matches "{label}: {X.X}ms" format

5. test_timer_returns_self:
   - Use "with PerfTimer() as timer:"
   - Verify timer is the PerfTimer instance

6. test_timer_handles_exception:
   - Use context that raises exception
   - Verify elapsed_ms still set (timer completes)
   - Verify exception propagates

Pattern:
```python
import logging
import time
from unittest.mock import MagicMock

from wanctl.perf_profiler import PerfTimer, OperationProfiler, measure_operation


class TestPerfTimer:
    def test_timer_measures_elapsed_time(self):
        with PerfTimer("test", None) as timer:
            time.sleep(0.01)
        assert timer.elapsed_ms > 0
        assert timer.elapsed_ms < 100  # Sanity check

    def test_timer_with_logger(self):
        mock_logger = MagicMock(spec=logging.Logger)
        with PerfTimer("test_operation", mock_logger):
            time.sleep(0.001)
        mock_logger.debug.assert_called_once()
        call_arg = mock_logger.debug.call_args[0][0]
        assert "test_operation:" in call_arg
        assert "ms" in call_arg
```
  </action>
  <verify>`.venv/bin/pytest tests/test_perf_profiler.py::TestPerfTimer -v` passes</verify>
  <done>PerfTimer context manager fully tested including timing, logging, and exception handling</done>
</task>

<task type="auto">
  <name>Task 2: Add OperationProfiler tests</name>
  <files>tests/test_perf_profiler.py</files>
  <action>
Add TestOperationProfiler class.

**TestOperationProfiler class:**

1. test_record_creates_sample_deque:
   - Create OperationProfiler
   - Record a sample with label
   - Verify label in profiler.samples
   - Verify sample value in deque

2. test_record_respects_max_samples:
   - Create OperationProfiler(max_samples=5)
   - Record 10 samples for same label
   - Verify only 5 samples retained

3. test_stats_empty_label:
   - Create OperationProfiler
   - Call stats("nonexistent")
   - Verify returns empty dict

4. test_stats_single_sample:
   - Record single sample (50.0)
   - Call stats(label)
   - Verify min=max=avg=p95=p99=50.0, count=1

5. test_stats_multiple_samples:
   - Record samples [10, 20, 30, 40, 50]
   - Call stats(label)
   - Verify min=10, max=50, avg=30
   - Verify p95 and p99 calculated correctly

6. test_stats_returns_samples_list:
   - Record samples
   - Call stats(label)
   - Verify "samples" key contains list of recorded values

7. test_clear_specific_label:
   - Record samples for "label_a" and "label_b"
   - Call clear("label_a")
   - Verify "label_a" cleared, "label_b" retained

8. test_clear_all:
   - Record samples for multiple labels
   - Call clear(None)
   - Verify all samples cleared

9. test_clear_nonexistent_label:
   - Call clear("nonexistent")
   - Verify no exception

10. test_report_no_data:
    - Create empty OperationProfiler
    - Call report()
    - Verify returns "No profiling data collected"

11. test_report_with_data:
    - Record samples for "op_a" and "op_b"
    - Call report()
    - Verify output contains headers and both labels
    - Verify format includes count, min, avg, max, p95, p99

12. test_report_logs_to_logger:
    - Create mock logger
    - Record samples
    - Call report(logger)
    - Verify logger.info called with report string

Pattern:
```python
class TestOperationProfiler:
    def test_record_creates_sample_deque(self):
        profiler = OperationProfiler(max_samples=100)
        profiler.record("test_op", 25.5)
        assert "test_op" in profiler.samples
        assert 25.5 in profiler.samples["test_op"]

    def test_stats_multiple_samples(self):
        profiler = OperationProfiler()
        for val in [10.0, 20.0, 30.0, 40.0, 50.0]:
            profiler.record("op", val)
        stats = profiler.stats("op")
        assert stats["count"] == 5
        assert stats["min_ms"] == 10.0
        assert stats["max_ms"] == 50.0
        assert stats["avg_ms"] == 30.0
```
  </action>
  <verify>`.venv/bin/pytest tests/test_perf_profiler.py::TestOperationProfiler -v` passes</verify>
  <done>OperationProfiler fully tested including record, stats, clear, and report methods</done>
</task>

<task type="auto">
  <name>Task 3: Add measure_operation decorator tests</name>
  <files>tests/test_perf_profiler.py</files>
  <action>
Add TestMeasureOperationDecorator class.

**TestMeasureOperationDecorator class:**

1. test_decorator_returns_function_result:
   - Create simple function that returns a value
   - Apply measure_operation decorator
   - Call decorated function
   - Verify original return value

2. test_decorator_times_execution:
   - Create function with sleep
   - Apply measure_operation with mock logger
   - Call decorated function
   - Verify logger.debug called with timing

3. test_decorator_without_logger:
   - Create function
   - Apply measure_operation with logger=None
   - Call decorated function
   - Verify no exception, returns result

4. test_decorator_passes_args:
   - Create function that uses args and kwargs
   - Apply measure_operation
   - Call with args and kwargs
   - Verify function received correct arguments

5. test_decorator_handles_exception:
   - Create function that raises exception
   - Apply measure_operation
   - Verify exception propagates
   - Verify timing still logged (before exception)

Pattern:
```python
class TestMeasureOperationDecorator:
    def test_decorator_returns_function_result(self):
        @measure_operation
        def sample_func(x, y):
            return x + y

        # Need to wrap manually since decorator needs label
        def sample_func(x, y):
            return x + y
        wrapped = measure_operation(sample_func, "add", None)
        result = wrapped(2, 3)
        assert result == 5

    def test_decorator_times_execution(self):
        mock_logger = MagicMock(spec=logging.Logger)

        def slow_func():
            time.sleep(0.01)
            return "done"

        wrapped = measure_operation(slow_func, "slow_op", mock_logger)
        result = wrapped()

        assert result == "done"
        mock_logger.debug.assert_called_once()
        assert "slow_op:" in mock_logger.debug.call_args[0][0]

    def test_decorator_passes_args_and_kwargs(self):
        def func_with_args(a, b, c=None):
            return f"{a}-{b}-{c}"

        wrapped = measure_operation(func_with_args, "test", None)
        result = wrapped("x", "y", c="z")
        assert result == "x-y-z"
```
  </action>
  <verify>`.venv/bin/pytest tests/test_perf_profiler.py::TestMeasureOperationDecorator -v` passes</verify>
  <done>measure_operation decorator fully tested including argument passing and timing</done>
</task>

</tasks>

<verification>
Run full coverage check:
```bash
.venv/bin/pytest tests/test_perf_profiler.py --cov=wanctl.perf_profiler --cov-report=term-missing 2>&1 | grep -E "perf_profiler.py.*%"
```

Target: 90%+ coverage (from 0%)

Run all tests:
```bash
.venv/bin/pytest tests/test_perf_profiler.py -v
```
</verification>

<success_criteria>
1. TestPerfTimer: Context manager timing, logging, exception handling
2. TestOperationProfiler: record, stats, clear, report all tested
3. TestMeasureOperationDecorator: Function wrapping, timing, args passing
4. perf_profiler.py coverage >= 90%
5. All tests pass: `.venv/bin/pytest tests/test_perf_profiler.py -v`
</success_criteria>

<output>
After completion, create `.planning/phases/37-cli-tool-tests/37-02-SUMMARY.md`
</output>
